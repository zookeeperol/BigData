# hadoop集群搭配

### 0.准备工作

##### 修改linux主机名

hostnamectl set-hostname "hdp1"

hostnamectl set-hostname "hdp1" --static

hostnamectl set-hostname "hdp1" --transient

hostnamectl set-hostname "hdp1" --pretty



##### 修改网络映射(每台虚拟机都要配置)

vi /etc/hosts
192.168.56.101 hdp1

192.168.56.102 hdp2

192.168.56.103 hdp3

##### 修改win(C:\Windows\System32\drivers\etc\host 将上一步ip复制到host文件)



### 1.将jdk上传解压到linux目录下

​		tar -zxvf jdk -C /usr/local

### 2.配置jdk文件


```javascript

vi /etc/profile
JAVA_HOME=/usr/local/jdk1.8.0_192
PATH=$JAVA_HOME/bin:$PATH
export JAVA_HOME PATH
刷新 :source /etc/profile;
```


### 3.安装完全分布式hdfs

tar -zxvf hadoop.2.8.5.tar.gz -C /usr/local

### 4.将hadoop-env.sh 中的JAVA_HOME路径更改

vi /usr/local/hadoop-2.8.5/etc/hadoop/hadoop-env.sh 

export JAVA_HOME=/usr/local/jdk1.8.0_192

### 5.更改/usr/local/hadoop-2.8.5/etc/hadoop下core-site.xml中的配置

vi core-site.xml

此标签

<configuration>

加入

<property>
<name>fs.defaultFS</name>
<value>hdfs://hdp1/</value>
</property>

</configuration>

### 6.更改/usr/local/hadoop-2.8.5/etc/hadoop下hdfs-site.xml中的配置

vi hdfs-site.xml

此标签

<configuration>

加入

<property>
<name>dfs.namenode.name.dir</name>
<value>/usr/local/hdpdata/name</value>
</property>

<property>
<name>dfs.datanode.data.dir</name>
<value>/usr/local/hdpdata/data</value>
</property>



<property>
  <name>dfs.namenode.secondary.http-address</name>
  <value>hdp2:50090</value>
</property>

<property>
  <name>dfs.namenode.checkpoint.dir</name>
  <value>/usr/local/hdpdata/namesecondary</value>
</property>

</configuration>

### 7.修改/usr/local/hadoop-2.8.5/etc/hadoop下slaves

vi slaves

写入

hdp1
hdp2
hdp3

### 8.配置 vi /etc/profile
```javascript

JAVA_HOME=/usr/local/jdk1.8.0_192
HADOOP_HOME=/usr/local/hadoop-2.8.5
PATH=$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH
export JAVA_HOME HADOOP_HOME PATH;
```


### 9.分发profile

切换到cd /etc 下

执行 scp profile hdp2:$ PWD | scp profile hdp3:$ PWD(每台虚拟机都要分发)

### 10.分发hadoop进行安装

scp hadoop hdp_:$PWD

### 11.分发hadoop-env.sh/core-site.xml/hdfs-site.xml

scp hadoop-env.sh core-site.xml hdfs-site.xml hdp_:$PWD

### 12.免密登陆

ssh-keygen -t rsa

ssh-copy-id 192.168.56.10?(每台机器都要免密对自己也要配置)

### 13.在/usr/local下创建文件

mkdir hdpdata(每台虚拟机配置)

### 14.在/usr/local/hadoop-2.8.5/etc/hadoop执行 hadoop namenode -format

### 15.启动namenode

hadoop-daemon.sh start namenode

### 16.启动datanode

hadoop-daemon.sh start datanode

(其他只启动datanode)

### 18.关闭防火墙(没有关闭不显示页面)

查看防火墙状态
firewall-cmd --state
关闭防火墙
systemctl stop firewalld.service
永久禁用防火墙
systemctl disable firewalld.service

# yarn配置

### 1.将/usr/local/hadoop-2.8.5/etc/hadoop目录下mapred-site.xml复制重命名

cp mapred-site.xml.template mapred-site.xml

### 2.修改mapred-site.xml

vi mapred-site.xml

<configuration>

<property>
<name>mapreduce.framework.name</name>
<value>yarn</value>
</property>

</configuration>

### 3.修改/usr/local/hadoop-2.8.5/etc/hadoop目录下yarn-site.xml

vi yarn-site.xml 

<configuration>

<property>
<name>yarn.resourcemanager.hostname</name>
<value>hdp01</value>
</property>

<property>
<name>yarn.nodemanager.aux-services</name>
<value>mapreduce_shuffle</value>
</property>



</configuration>



### 4.分发yarn-site.xml  mapred-site.xml

scp yarn-site.xml  mapred-site.xml hdp_:$PWD



### 5.启动start-yarn.sh
